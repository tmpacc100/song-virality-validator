#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ML/RLãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’è¤‡æ•°ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰çµ±åˆ
"""

import json
import csv
import pandas as pd
from datetime import datetime
import os


class DataIntegrator:
    """è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’çµ±åˆã—ã¦ML/RLç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""

    def __init__(self):
        self.youtube_api_data = []
        self.channel_history_data = []
        self.taiko_release_data = []
        self.rankings_data = []

    def load_all_sources(self):
        """ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿"""
        print("="*60)
        print("ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹èª­ã¿è¾¼ã¿")
        print("="*60)

        # 1. YouTube API raw data
        try:
            with open('RAW DATA/Youtube_API_raw.json', 'r', encoding='utf-8') as f:
                self.youtube_api_data = json.load(f)
            print(f"âœ“ Youtube_API_raw.json: {len(self.youtube_api_data)}ä»¶")
        except Exception as e:
            print(f"âœ— Youtube_API_raw.json: {e}")

        # 2. Channel history
        try:
            with open('channel_history.json', 'r', encoding='utf-8') as f:
                self.channel_history_data = json.load(f)
            print(f"âœ“ channel_history.json: {len(self.channel_history_data)}ä»¶")
        except Exception as e:
            print(f"âœ— channel_history.json: {e}")

        # 3. TaikoGame ãƒªãƒªãƒ¼ã‚¹ãƒ»é–‹ç™ºä¸­ãƒ‡ãƒ¼ã‚¿
        try:
            with open('filtered data/taiko_server_ãƒªãƒªãƒ¼ã‚¹_é–‹ç™ºä¸­_filtered.csv', 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                self.taiko_release_data = list(reader)
            print(f"âœ“ taiko_server_ãƒªãƒªãƒ¼ã‚¹_é–‹ç™ºä¸­_filtered.csv: {len(self.taiko_release_data)}ä»¶")
        except Exception as e:
            print(f"âœ— taiko_server_ãƒªãƒªãƒ¼ã‚¹_é–‹ç™ºä¸­_filtered.csv: {e}")

        # 4. æ—¢å­˜rankings.json
        try:
            with open('rankings.json', 'r', encoding='utf-8') as f:
                rankings = json.load(f)
                if 'overall' in rankings:
                    self.rankings_data = rankings['overall']
            print(f"âœ“ rankings.json: {len(self.rankings_data)}ä»¶")
        except Exception as e:
            print(f"âœ— rankings.json: {e}")

        print()

    def integrate_data(self):
        """ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¦è¨“ç·´ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
        print("="*60)
        print("ãƒ‡ãƒ¼ã‚¿çµ±åˆ")
        print("="*60)

        integrated_data = []

        # video_idã‚’ã‚­ãƒ¼ã«ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
        video_id_map = {}

        # 1. YouTube API dataã‚’ãƒ™ãƒ¼ã‚¹ã«
        for item in self.youtube_api_data:
            video_id = item.get('video_id', '')
            if video_id:
                video_id_map[video_id] = {
                    'video_id': video_id,
                    'song_name': item.get('song_name', ''),
                    'artist_name': item.get('artist_name', ''),
                    'release_date': item.get('release_date', ''),
                    'video_title': item.get('video_title', ''),
                    'channel_title': item.get('channel_title', ''),
                    'view_count': item.get('view_count', 0),
                    'like_count': item.get('like_count', 0),
                    'comment_count': item.get('comment_count', 0),
                    'support_rate': item.get('support_rate', 0),
                    'growth_rate': item.get('growth_rate', 0),
                    'days_since_published': item.get('days_since_published', 0),
                    'source': 'youtube_api'
                }

        print(f"  YouTube API: {len(video_id_map)}ä»¶ã‚’ãƒ™ãƒ¼ã‚¹ã«è¨­å®š")

        # 2. Channel historyã¯è‡ªåˆ†ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®æŠ•ç¨¿å±¥æ­´ï¼ˆåˆ¥ãƒ‡ãƒ¼ã‚¿ï¼‰
        # ã“ã‚Œã‚‰ã¯ç‹¬ç«‹ã—ãŸã‚µãƒ³ãƒ—ãƒ«ã¨ã—ã¦è¿½åŠ 
        channel_history_added = 0
        for item in self.channel_history_data:
            video_id = item.get('video_id', '')
            if video_id and video_id not in video_id_map:
                # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æ›²åã‚’æŠ½å‡º
                title = item.get('title', '')

                # æ–°ã—ã„ã‚¨ãƒ³ãƒˆãƒªã¨ã—ã¦è¿½åŠ 
                video_id_map[video_id] = {
                    'video_id': video_id,
                    'song_name': title,  # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ›²åã¨ã—ã¦ä½¿ç”¨
                    'artist_name': 'ãŸã„ã“ã§ãƒ’ãƒƒãƒˆã‚½ãƒ³ã‚°',  # è‡ªåˆ†ã®ãƒãƒ£ãƒ³ãƒãƒ«å
                    'release_date': '',
                    'video_title': title,
                    'channel_title': 'ãŸã„ã“ã§ãƒ’ãƒƒãƒˆã‚½ãƒ³ã‚°',
                    'view_count': item.get('view_count', 0),
                    'like_count': item.get('like_count', 0),
                    'comment_count': item.get('comment_count', 0),
                    'support_rate': 0,
                    'growth_rate': 0,
                    'days_since_published': 0,
                    'published_at': item.get('published_at', ''),
                    'published_date': item.get('published_date', ''),
                    'published_time': item.get('published_time', ''),
                    'published_hour': item.get('published_hour', 0),
                    'published_day_of_week': item.get('published_day_of_week', ''),
                    'published_is_weekend': item.get('published_is_weekend', False),
                    'duration_seconds': item.get('duration_seconds', 0),
                    'is_short': item.get('is_short', False),
                    'tags': item.get('tags', ''),
                    'source': 'channel_history'
                }
                channel_history_added += 1

        print(f"  Channel history: {channel_history_added}ä»¶ã‚’è¿½åŠ ï¼ˆç‹¬ç«‹ã‚µãƒ³ãƒ—ãƒ«ï¼‰")

        # 3. TaikoGameãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¿ã‚°ãƒ»é›£æ˜“åº¦æƒ…å ±ã‚’è¿½åŠ 
        # song_nameã‚’ã‚­ãƒ¼ã«ãƒãƒƒãƒãƒ³ã‚°
        song_name_to_taiko = {}
        for item in self.taiko_release_data:
            song_name = item.get('song_name', '').strip()
            if song_name:
                song_name_to_taiko[song_name] = item

        taiko_matched = 0
        for video_id, data in video_id_map.items():
            song_name = data.get('song_name', '')
            if song_name in song_name_to_taiko:
                taiko_item = song_name_to_taiko[song_name]
                data.update({
                    'taiko_id': taiko_item.get('id', ''),
                    'tags': taiko_item.get('tags', ''),
                    'difficulty': taiko_item.get('difficulty', ''),
                    'release_status': taiko_item.get('release_status', ''),
                })
                taiko_matched += 1

        print(f"  TaikoGame data: {taiko_matched}ä»¶ã‚’ãƒãƒƒãƒãƒ³ã‚°")

        # 4. rankings.jsonã‹ã‚‰è¿½åŠ ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’çµ±åˆ
        rankings_matched = 0
        for item in self.rankings_data:
            video_id = item.get('video_id', '')
            if video_id in video_id_map:
                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ï¼ˆã‚ˆã‚Šæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆï¼‰
                if 'metrics' in item:
                    video_id_map[video_id].update({
                        'view_count': item['metrics'].get('view_count', video_id_map[video_id]['view_count']),
                        'like_count': item['metrics'].get('like_count', video_id_map[video_id]['like_count']),
                        'comment_count': item['metrics'].get('comment_count', video_id_map[video_id]['comment_count']),
                        'support_rate': item['metrics'].get('support_rate', video_id_map[video_id]['support_rate']),
                        'growth_rate': item['metrics'].get('growth_rate', video_id_map[video_id]['growth_rate']),
                    })
                rankings_matched += 1

        print(f"  Rankings.json: {rankings_matched}ä»¶ã‚’ãƒãƒƒãƒãƒ³ã‚°")

        # ãƒªã‚¹ãƒˆã«å¤‰æ›
        integrated_data = list(video_id_map.values())

        print()
        print(f"âœ“ çµ±åˆå®Œäº†: {len(integrated_data)}ä»¶")
        print()

        return integrated_data

    def save_integrated_data(self, data, output_path='ML_training_data.json'):
        """çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        print("="*60)
        print("çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜")
        print("="*60)

        # JSONå½¢å¼ã§ä¿å­˜
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"âœ“ JSONä¿å­˜: {output_path} ({len(data)}ä»¶)")

        # CSVå½¢å¼ã§ã‚‚ä¿å­˜
        csv_path = output_path.replace('.json', '.csv')
        if data:
            # å…¨ã¦ã®ã‚­ãƒ¼ã‚’åé›†ï¼ˆãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã£ã¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒç•°ãªã‚‹å ´åˆãŒã‚ã‚‹ãŸã‚ï¼‰
            all_keys = set()
            for item in data:
                all_keys.update(item.keys())

            with open(csv_path, 'w', encoding='utf-8', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=sorted(all_keys))
                writer.writeheader()
                writer.writerows(data)

            print(f"âœ“ CSVä¿å­˜: {csv_path}")

        print()

    def analyze_integrated_data(self, data):
        """çµ±åˆãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
        print("="*60)
        print("çµ±åˆãƒ‡ãƒ¼ã‚¿åˆ†æ")
        print("="*60)

        if not data:
            print("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return

        # åŸºæœ¬çµ±è¨ˆ
        print(f"\nç·ä»¶æ•°: {len(data)}ä»¶\n")

        # æŠ•ç¨¿æ—¥æ™‚ãƒ‡ãƒ¼ã‚¿ã®æœ‰ç„¡
        has_published_at = sum(1 for item in data if item.get('published_at'))
        print(f"æŠ•ç¨¿æ—¥æ™‚ãƒ‡ãƒ¼ã‚¿: {has_published_at}ä»¶ ({has_published_at/len(data)*100:.1f}%)")

        # ã‚¿ã‚°ãƒ‡ãƒ¼ã‚¿ã®æœ‰ç„¡
        has_tags = sum(1 for item in data if item.get('tags'))
        print(f"ã‚¿ã‚°ãƒ‡ãƒ¼ã‚¿: {has_tags}ä»¶ ({has_tags/len(data)*100:.1f}%)")

        # release_dateã®æœ‰ç„¡
        has_release_date = sum(1 for item in data if item.get('release_date'))
        print(f"release_date: {has_release_date}ä»¶ ({has_release_date/len(data)*100:.1f}%)")

        # è¦–è´æ•°çµ±è¨ˆ
        view_counts = [item.get('view_count', 0) for item in data if item.get('view_count')]
        if view_counts:
            import statistics
            print(f"\nè¦–è´æ•°çµ±è¨ˆ:")
            print(f"  å¹³å‡: {statistics.mean(view_counts):,.0f}")
            print(f"  ä¸­å¤®å€¤: {statistics.median(view_counts):,.0f}")
            print(f"  æœ€å°: {min(view_counts):,.0f}")
            print(f"  æœ€å¤§: {max(view_counts):,.0f}")

        print()


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    import sys

    integrator = DataIntegrator()

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    integrator.load_all_sources()

    # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
    integrated_data = integrator.integrate_data()

    # YouTube API ã‚¨ãƒ³ãƒªãƒƒãƒãƒ¡ãƒ³ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    print("="*60)
    print("YouTube API ã‚¨ãƒ³ãƒªãƒƒãƒãƒ¡ãƒ³ãƒˆ")
    print("="*60)
    user_input = input("YouTube APIã§ãƒ‡ãƒ¼ã‚¿ã‚’è£œå¼·ã—ã¾ã™ã‹ï¼Ÿ (y/n): ").strip().lower()

    if user_input == 'y':
        try:
            # YouTube API Enricherã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            from youtube_api_enricher import YouTubeAPIEnricher
            from main import YOUTUBE_API_KEYS

            enricher = YouTubeAPIEnricher(YOUTUBE_API_KEYS)

            print("\nğŸ“Š ã‚¨ãƒ³ãƒªãƒƒãƒãƒ¡ãƒ³ãƒˆè¨­å®š:")
            print("  - å‹•ç”»è©³ç´°ãƒ‡ãƒ¼ã‚¿: ON")
            print("  - ãƒãƒ£ãƒ³ãƒãƒ«ãƒ‡ãƒ¼ã‚¿: ON")
            print("  - é–¢é€£å‹•ç”»ãƒ‡ãƒ¼ã‚¿: OFF")
            print()

            # ã‚¨ãƒ³ãƒªãƒƒãƒãƒ¡ãƒ³ãƒˆå®Ÿè¡Œ
            integrated_data = enricher.enrich_dataset(
                integrated_data,
                include_channel=True,
                include_related=False,
                verbose=True
            )

            print("âœ… YouTube APIã‚¨ãƒ³ãƒªãƒƒãƒãƒ¡ãƒ³ãƒˆå®Œäº†")
            print()

        except ImportError as e:
            print(f"âš  youtube_api_enricher.py ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
            print("  ã‚¨ãƒ³ãƒªãƒƒãƒãƒ¡ãƒ³ãƒˆãªã—ã§ç¶šè¡Œã—ã¾ã™")
        except Exception as e:
            print(f"âš  ã‚¨ãƒ³ãƒªãƒƒãƒãƒ¡ãƒ³ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            print("  ã‚¨ãƒ³ãƒªãƒƒãƒãƒ¡ãƒ³ãƒˆãªã—ã§ç¶šè¡Œã—ã¾ã™")
    else:
        print("â­ ã‚¨ãƒ³ãƒªãƒƒãƒãƒ¡ãƒ³ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—")
        print()

    # YouTubeã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹çµ±åˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    print("="*60)
    print("YouTubeã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹çµ±åˆ")
    print("="*60)
    analytics_input = input("YouTubeã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¾ã™ã‹ï¼Ÿ (y/n): ").strip().lower()

    if analytics_input == 'y':
        try:
            from youtube_analytics_integrator import YouTubeAnalyticsIntegrator

            analytics_integrator = YouTubeAnalyticsIntegrator()
            analytics_integrator.load_all_analytics()

            # åºƒå‘Šãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filter_ads_input = input("åºƒå‘Šå‹•ç”»ã‚’é™¤å¤–ã—ã¾ã™ã‹ï¼Ÿ (y/n, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: y): ").strip().lower() or 'y'
            filter_ads = (filter_ads_input == 'y')

            integrated_data = analytics_integrator.enrich_ml_training_data(
                integrated_data,
                filter_ads=filter_ads
            )

            print("âœ… YouTubeã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹çµ±åˆå®Œäº†")
            print()

        except ImportError as e:
            print(f"âš  youtube_analytics_integrator.py ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
            print("  ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹çµ±åˆãªã—ã§ç¶šè¡Œã—ã¾ã™")
        except Exception as e:
            print(f"âš  ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹çµ±åˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            print("  ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹çµ±åˆãªã—ã§ç¶šè¡Œã—ã¾ã™")
    else:
        print("â­ ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹çµ±åˆã‚’ã‚¹ã‚­ãƒƒãƒ—")
        print()

    # çµ±è¨ˆåˆ†æ
    integrator.analyze_integrated_data(integrated_data)

    # ä¿å­˜
    integrator.save_integrated_data(integrated_data)

    print("="*60)
    print("âœ… å®Œäº†")
    print("="*60)
    print()
    print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("  1. ML_training_data.json ã‚’ç¢ºèª")
    print("  2. main.py ã§ã‚ªãƒ—ã‚·ãƒ§ãƒ³10ã‚’å®Ÿè¡Œã—ã¦ML/RLè¨“ç·´")
    print()


if __name__ == '__main__':
    main()
