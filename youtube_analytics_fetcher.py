#!/usr/bin/env python3
"""
YouTube Analytics API ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
è‡ªåˆ†ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®è©³ç´°ãªæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦MLç²¾åº¦å‘ä¸Š

å¿…è¦ãªè¨­å®š:
1. Google Cloud Console ã§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
2. YouTube Analytics API ã‚’æœ‰åŠ¹åŒ–
3. OAuth 2.0 ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆIDä½œæˆï¼ˆãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã‚¢ãƒ—ãƒªï¼‰
4. credentials.json ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã«é…ç½®

åˆå›å®Ÿè¡Œæ™‚ã«ãƒ–ãƒ©ã‚¦ã‚¶ã§èªè¨¼ãŒå¿…è¦ã§ã™ã€‚
èªè¨¼å¾Œã¯ token.json ãŒä½œæˆã•ã‚Œã€æ¬¡å›ä»¥é™ã¯è‡ªå‹•èªè¨¼ã•ã‚Œã¾ã™ã€‚
"""

import os
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError


# OAuth 2.0 ã‚¹ã‚³ãƒ¼ãƒ—
SCOPES = [
    'https://www.googleapis.com/auth/youtube.readonly',
    'https://www.googleapis.com/auth/yt-analytics.readonly'
]


class YouTubeAnalyticsFetcher:
    """YouTube Analytics API ã§ãƒãƒ£ãƒ³ãƒãƒ«ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""

    def __init__(self, credentials_path: str = 'credentials.json',
                 token_path: str = 'token.json'):
        """
        Args:
            credentials_path: OAuth 2.0 ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆID JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            token_path: èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ä¿å­˜å…ˆ
        """
        self.credentials_path = credentials_path
        self.token_path = token_path
        self.youtube = None
        self.youtube_analytics = None
        self.channel_id = None

    def authenticate(self):
        """OAuth 2.0 èªè¨¼ã‚’å®Ÿè¡Œ"""
        creds = None

        # ä¿å­˜ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’èª­ã¿è¾¼ã¿
        if os.path.exists(self.token_path):
            creds = Credentials.from_authorized_user_file(self.token_path, SCOPES)

        # ãƒˆãƒ¼ã‚¯ãƒ³ãŒç„¡åŠ¹ã¾ãŸã¯å­˜åœ¨ã—ãªã„å ´åˆã€æ–°è¦èªè¨¼
        if not creds or not creds.valid:
            if creds and creds.expired and creds.refresh_token:
                print("ğŸ”„ ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ›´æ–°ä¸­...")
                creds.refresh(Request())
            else:
                if not os.path.exists(self.credentials_path):
                    print("âŒ credentials.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    print("\nã€è¨­å®šæ‰‹é †ã€‘")
                    print("1. https://console.cloud.google.com/ ã«ã‚¢ã‚¯ã‚»ã‚¹")
                    print("2. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ")
                    print("3. YouTube Analytics API ã‚’æœ‰åŠ¹åŒ–")
                    print("4. èªè¨¼æƒ…å ± â†’ OAuth 2.0 ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆID ä½œæˆï¼ˆãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã‚¢ãƒ—ãƒªï¼‰")
                    print("5. credentials.json ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã“ã®ãƒ•ã‚©ãƒ«ãƒ€ã«é…ç½®")
                    print()
                    return False

                print("ğŸ” åˆå›èªè¨¼ã‚’é–‹å§‹...")
                print("ãƒ–ãƒ©ã‚¦ã‚¶ãŒé–‹ãã¾ã™ã€‚Googleã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„ã€‚")
                flow = InstalledAppFlow.from_client_secrets_file(
                    self.credentials_path, SCOPES)
                creds = flow.run_local_server(port=0)

            # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¿å­˜
            with open(self.token_path, 'w') as token:
                token.write(creds.to_json())
            print("âœ… èªè¨¼æˆåŠŸ")

        # YouTube Data API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        self.youtube = build('youtube', 'v3', credentials=creds)

        # YouTube Analytics API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        self.youtube_analytics = build('youtubeAnalytics', 'v2', credentials=creds)

        # è‡ªåˆ†ã®ãƒãƒ£ãƒ³ãƒãƒ«IDã‚’å–å¾—
        self._get_channel_id()

        return True

    def _get_channel_id(self):
        """è‡ªåˆ†ã®ãƒãƒ£ãƒ³ãƒãƒ«IDã‚’å–å¾—"""
        try:
            request = self.youtube.channels().list(
                part='id,snippet',
                mine=True
            )
            response = request.execute()

            if response['items']:
                self.channel_id = response['items'][0]['id']
                channel_title = response['items'][0]['snippet']['title']
                print(f"âœ… ãƒãƒ£ãƒ³ãƒãƒ«èªè­˜: {channel_title} (ID: {self.channel_id})")
            else:
                print("âš  ãƒãƒ£ãƒ³ãƒãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        except HttpError as e:
            print(f"âŒ ãƒãƒ£ãƒ³ãƒãƒ«IDå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

    def fetch_channel_videos(self, max_results: int = 500) -> List[Dict[str, Any]]:
        """è‡ªåˆ†ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®å…¨å‹•ç”»ã‚’å–å¾—

        Args:
            max_results: å–å¾—ã™ã‚‹æœ€å¤§å‹•ç”»æ•°

        Returns:
            å‹•ç”»æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        if not self.channel_id:
            print("âš  ãƒãƒ£ãƒ³ãƒãƒ«IDãŒå–å¾—ã§ãã¦ã„ã¾ã›ã‚“")
            return []

        print("\n" + "=" * 60)
        print("ãƒãƒ£ãƒ³ãƒãƒ«å‹•ç”»ä¸€è¦§ã‚’å–å¾—ä¸­...")
        print("=" * 60)

        videos = []
        next_page_token = None

        while len(videos) < max_results:
            try:
                # è‡ªåˆ†ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‹•ç”»ã‚’æ¤œç´¢
                request = self.youtube.search().list(
                    part='id,snippet',
                    channelId=self.channel_id,
                    type='video',
                    order='date',
                    maxResults=min(50, max_results - len(videos)),
                    pageToken=next_page_token
                )
                response = request.execute()

                for item in response['items']:
                    video_id = item['id']['videoId']
                    snippet = item['snippet']

                    videos.append({
                        'video_id': video_id,
                        'title': snippet['title'],
                        'published_at': snippet['publishedAt'],
                        'description': snippet['description'],
                        'thumbnail_url': snippet['thumbnails'].get('high', {}).get('url', ''),
                    })

                next_page_token = response.get('nextPageToken')

                if not next_page_token:
                    break

                print(f"  å–å¾—æ¸ˆã¿: {len(videos)}ä»¶")

            except HttpError as e:
                print(f"âŒ å‹•ç”»å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                break

        print(f"âœ… åˆè¨ˆ {len(videos)}ä»¶ã®å‹•ç”»ã‚’å–å¾—")
        return videos

    def fetch_time_series_data(self, video_id: str,
                               days_back: int = 30) -> Dict[str, Any]:
        """1ã¤ã®å‹•ç”»ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

        Args:
            video_id: YouTubeå‹•ç”»ID
            days_back: ä½•æ—¥å‰ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã‹

        Returns:
            æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼ˆæ—¥åˆ¥ã®å†ç”Ÿæ•°ã€é«˜è©•ä¾¡æ•°ç­‰ï¼‰
        """
        if not self.channel_id:
            return {}

        end_date = datetime.now().date()
        start_date = end_date - timedelta(days=days_back)

        try:
            # Analytics API ã§ãƒ¬ãƒãƒ¼ãƒˆã‚’å–å¾—
            request = self.youtube_analytics.reports().query(
                ids=f'channel=={self.channel_id}',
                startDate=start_date.isoformat(),
                endDate=end_date.isoformat(),
                metrics='views,likes,comments,estimatedMinutesWatched,averageViewDuration',
                dimensions='day',
                filters=f'video=={video_id}',
                sort='day'
            )
            response = request.execute()

            # ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
            time_series = {
                'video_id': video_id,
                'start_date': start_date.isoformat(),
                'end_date': end_date.isoformat(),
                'daily_data': []
            }

            if 'rows' in response:
                for row in response['rows']:
                    time_series['daily_data'].append({
                        'date': row[0],
                        'views': row[1],
                        'likes': row[2],
                        'comments': row[3],
                        'watch_time_minutes': row[4],
                        'average_view_duration': row[5],
                    })

            return time_series

        except HttpError as e:
            print(f"âš  {video_id}: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def fetch_traffic_source_data(self, video_id: str,
                                  days_back: int = 30) -> Dict[str, Any]:
        """ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚½ãƒ¼ã‚¹ï¼ˆæµå…¥å…ƒï¼‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

        Args:
            video_id: YouTubeå‹•ç”»ID
            days_back: ä½•æ—¥å‰ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã‹

        Returns:
            ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚½ãƒ¼ã‚¹åˆ¥ã®å†ç”Ÿæ•°
        """
        if not self.channel_id:
            return {}

        end_date = datetime.now().date()
        start_date = end_date - timedelta(days=days_back)

        try:
            request = self.youtube_analytics.reports().query(
                ids=f'channel=={self.channel_id}',
                startDate=start_date.isoformat(),
                endDate=end_date.isoformat(),
                metrics='views',
                dimensions='insightTrafficSourceType',
                filters=f'video=={video_id}',
                sort='-views'
            )
            response = request.execute()

            traffic_sources = {
                'video_id': video_id,
                'sources': {}
            }

            if 'rows' in response:
                total_views = sum(row[1] for row in response['rows'])
                for row in response['rows']:
                    source_type = row[0]
                    views = row[1]
                    percentage = round(views / total_views * 100, 2) if total_views > 0 else 0

                    traffic_sources['sources'][source_type] = {
                        'views': views,
                        'percentage': percentage
                    }

            return traffic_sources

        except HttpError as e:
            print(f"âš  {video_id}: ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚½ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def fetch_all_analytics(self, max_videos: int = 500,
                           days_back: int = 90,
                           include_traffic_sources: bool = True) -> List[Dict[str, Any]]:
        """ãƒãƒ£ãƒ³ãƒãƒ«ã®å…¨å‹•ç”»ã®è©³ç´°ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹ã‚’å–å¾—

        Args:
            max_videos: å–å¾—ã™ã‚‹æœ€å¤§å‹•ç”»æ•°
            days_back: ä½•æ—¥å‰ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã‹
            include_traffic_sources: ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚‚å–å¾—ã™ã‚‹ã‹

        Returns:
            å…¨å‹•ç”»ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆ
        """
        # ã¾ãšå‹•ç”»ä¸€è¦§ã‚’å–å¾—
        videos = self.fetch_channel_videos(max_videos)

        if not videos:
            return []

        print("\n" + "=" * 60)
        print("è©³ç´°ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹ã‚’å–å¾—ä¸­...")
        print("=" * 60)

        analytics_data = []

        for idx, video in enumerate(videos, 1):
            video_id = video['video_id']
            title = video['title']

            if idx % 10 == 0:
                print(f"[{idx}/{len(videos)}] å‡¦ç†ä¸­...")

            # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            time_series = self.fetch_time_series_data(video_id, days_back)

            # ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚½ãƒ¼ã‚¹ã‚’å–å¾—
            traffic_sources = {}
            if include_traffic_sources:
                traffic_sources = self.fetch_traffic_source_data(video_id, days_back)

            # çµ±åˆ
            analytics_item = {
                **video,
                'time_series': time_series,
                'traffic_sources': traffic_sources
            }

            analytics_data.append(analytics_item)

        print(f"\nâœ… {len(analytics_data)}ä»¶ã®ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—å®Œäº†")

        return analytics_data

    def save_analytics_data(self, data: List[Dict[str, Any]],
                           output_path: str = 'youtube_analytics_data.json'):
        """ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

        Args:
            data: ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿
            output_path: ä¿å­˜å…ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {output_path}")
        print(f"   {len(data)}ä»¶ã®ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("=" * 60)
    print("YouTube Analytics ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ„ãƒ¼ãƒ«")
    print("=" * 60)
    print()

    fetcher = YouTubeAnalyticsFetcher()

    # èªè¨¼
    if not fetcher.authenticate():
        print("âŒ èªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸã€‚credentials.json ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return

    print()
    print("=" * 60)
    print("ãƒ‡ãƒ¼ã‚¿å–å¾—è¨­å®š")
    print("=" * 60)

    # è¨­å®šå…¥åŠ›
    try:
        max_videos = int(input("å–å¾—ã™ã‚‹å‹•ç”»æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 500ï¼‰: ").strip() or "500")
        days_back = int(input("ä½•æ—¥å‰ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼Ÿï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 90ï¼‰: ").strip() or "90")
        include_traffic = input("ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚½ãƒ¼ã‚¹ã‚‚å–å¾—ï¼Ÿ (y/n, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: y): ").strip().lower() or "y"
        include_traffic_sources = (include_traffic == 'y')
    except ValueError:
        print("âš  å…¥åŠ›ã‚¨ãƒ©ãƒ¼ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        max_videos = 500
        days_back = 90
        include_traffic_sources = True

    print()
    print("=" * 60)
    print("ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
    print("=" * 60)
    print(f"  å¯¾è±¡å‹•ç”»æ•°: {max_videos}ä»¶")
    print(f"  å–å¾—æœŸé–“: éå»{days_back}æ—¥")
    print(f"  ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚½ãƒ¼ã‚¹: {'ON' if include_traffic_sources else 'OFF'}")
    print()

    # ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹å–å¾—
    analytics_data = fetcher.fetch_all_analytics(
        max_videos=max_videos,
        days_back=days_back,
        include_traffic_sources=include_traffic_sources
    )

    # ä¿å­˜
    if analytics_data:
        output_path = 'RAW DATA/youtube_analytics_data.json'
        os.makedirs('RAW DATA', exist_ok=True)
        fetcher.save_analytics_data(analytics_data, output_path)

        print()
        print("=" * 60)
        print("âœ… å®Œäº†")
        print("=" * 60)
        print()
        print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("  1. RAW DATA/youtube_analytics_data.json ã‚’ç¢ºèª")
        print("  2. data_integrator.py ã§ML_training_data.json ã«çµ±åˆ")
        print("  3. feature_engineering.py ã«æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚’è¿½åŠ ")
        print("  4. ml_scheduler.py ã§LSTMãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´")
    else:
        print("âš  ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")


if __name__ == '__main__':
    main()
